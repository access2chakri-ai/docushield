"""
DocuShield Digital Twin Document Intelligence API
Clean, structured FastAPI implementation with multi-cluster TiDB and LLM Factory
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text, select
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json
import hashlib
import io
import logging
import time
import asyncio

# Document processing imports
import PyPDF2
import docx

# Internal imports
from app.database import get_operational_db, get_sandbox_db, get_analytics_db, ClusterType, init_db
from app.models import (
    User, BronzeContract, BronzeContractTextRaw, ProcessingRun,
    GoldContractScore, GoldFinding, GoldSuggestion, Alert
)
from app.services.document_processor import document_processor
from app.services.digital_twin import digital_twin_service, WorkflowType
from app.services.llm_factory import llm_factory, LLMProvider, LLMTask
from app.services.google_drive import google_drive_service
from app.services.external_integrations import external_integrations
from app.core.config import settings

logger = logging.getLogger(__name__)

# =============================================================================
# PYDANTIC MODELS
# =============================================================================

class DocumentUploadRequest(BaseModel):
    filename: str
    user_id: str
    source: str = "upload"

class ProcessContractRequest(BaseModel):
    contract_id: str
    user_id: str
    trigger: str = "manual"
    resume_from_step: Optional[str] = None

class SimulationRequest(BaseModel):
    scenario_name: str
    description: str
    document_ids: List[str]
    parameter_changes: Dict[str, Any]

class LLMRequest(BaseModel):
    prompt: str
    task_type: str = "completion"
    provider: Optional[str] = None
    max_tokens: int = 1000
    temperature: float = 0.7

class ContractAnalysisResponse(BaseModel):
    contract_id: str
    processing_run_id: str
    status: str
    overall_score: Optional[int]
    risk_level: Optional[str]
    findings_count: int
    suggestions_count: int

class DigitalTwinInsightsResponse(BaseModel):
    workflow_type: str
    metrics: Dict[str, Any]
    risk_patterns: List[Dict[str, Any]]
    recommendations: List[str]

# =============================================================================
# FASTAPI APP SETUP
# =============================================================================

app = FastAPI(
    title="DocuShield - Digital Twin Document Intelligence",
    description="Enterprise document analysis with multi-cluster TiDB and LLM Factory",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================================================
# STARTUP & HEALTH ENDPOINTS
# =============================================================================

@app.on_event("startup")
async def startup():
    """Initialize services and test connections"""
    logger.info("üöÄ Starting DocuShield Digital Twin Document Intelligence")
    
    # Auto-run database migrations on startup
    try:
        from migrations.migration_runner import MigrationRunner
        migration_runner = MigrationRunner()
        
        logger.info("üîÑ Checking for database migrations...")
        await migration_runner.migrate()
        logger.info("‚úÖ Database migrations completed")
    except Exception as e:
        logger.error(f"‚ùå Database migration failed: {e}")
        # Don't fail startup if migrations fail - log and continue
    
    # Initialize database tables (creates tables if they don't exist)
    try:
        await init_db()
        logger.info("‚úÖ Database tables verified")
    except Exception as e:
        logger.error(f"‚ùå Database initialization failed: {e}")
    
    # Test operational cluster
    try:
        async for db in get_operational_db():
            await db.execute(text("SELECT 1"))
            logger.info("‚úÖ Operational cluster connected")
            break
    except Exception as e:
        logger.error(f"‚ùå Operational cluster failed: {e}")
    
    # Test LLM Factory (skip for development)
    try:
        provider_status = llm_factory.get_provider_status()
        available_providers = [p for p, status in provider_status["providers"].items() if status["available"]]
        logger.info(f"‚úÖ LLM Factory initialized with providers: {available_providers}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è LLM Factory not available: {e}")

@app.get("/health")
async def health_check():
    """Comprehensive health check"""
    return {
        "status": "healthy",
        "service": "DocuShield Digital Twin Document Intelligence",
        "version": "2.0.0",
        "features": {
            "multi_cluster_tidb": True,
            "digital_twin_mapping": True,
            "risk_analysis": True,
            "google_drive_sync": bool(settings.google_drive_folder_id),
            "slack_alerts": bool(settings.slack_bot_token),
            "email_alerts": bool(settings.sendgrid_api_key)
        }
    }

@app.get("/api/capabilities")
async def get_system_capabilities():
    """Get comprehensive DocuShield system capabilities"""
    return {
        "service": "DocuShield - Digital Twin Document Intelligence",
        "version": "2.0.0",
        "architecture": "Bronze ‚Üí Silver ‚Üí Gold Data Pipeline",
        
        "document_support": {
            "supported_formats": {
                "pdf": {
                    "extension": ".pdf",
                    "mime_type": "application/pdf",
                    "capabilities": ["text_extraction", "analysis", "vector_search", "chat"],
                    "status": "‚úÖ Fully Supported"
                },
                "docx": {
                    "extension": ".docx",
                    "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    "capabilities": ["text_extraction", "analysis", "vector_search", "chat"],
                    "status": "‚úÖ Fully Supported"
                },
                "text": {
                    "extension": [".txt", ".md"],
                    "mime_type": "text/plain",
                    "capabilities": ["direct_analysis", "vector_search", "chat"],
                    "status": "‚úÖ Fully Supported"
                }
            },
            "unsupported_formats": {
                "images": ["jpg", "png", "gif", "bmp"],
                "spreadsheets": ["xlsx", "xls", "csv"],
                "presentations": ["pptx", "ppt"],
                "other": ["zip", "rar", "exe", "dmg"]
            }
        },
        
        "core_features": {
            "authentication": {
                "user_management": "‚úÖ Multi-user support with data isolation",
                "registration": "‚úÖ User registration and login",
                "security": "‚úÖ Document access control per user"
            },
            "document_processing": {
                "upload": "‚úÖ Secure file upload to TiDB LONGBLOB",
                "text_extraction": "‚úÖ PDF, DOCX, TXT processing",
                "deduplication": "‚úÖ SHA-256 hash-based duplicate detection",
                "storage": "‚úÖ TiDB multi-cluster storage"
            },
            "ai_capabilities": {
                "chat_interface": "‚úÖ Real-time document Q&A",
                "vector_search": "‚úÖ TiDB Vector Search integration",
                "llm_integration": "‚úÖ Multi-provider LLM support",
                "context_awareness": "‚úÖ User-specific document context"
            },
            "data_architecture": {
                "bronze_layer": "‚úÖ Raw document ingestion",
                "silver_layer": "‚úÖ Processed chunks and embeddings", 
                "gold_layer": "‚úÖ Analysis results and insights",
                "multi_cluster": "‚úÖ Operational, Sandbox, Analytics clusters"
            }
        },
        
        "current_limitations": {
            "document_types": "Limited to PDF, DOCX, TXT, MD files",
            "llm_analysis": "Simplified demo mode (full LLM integration available)",
            "vector_search": "Basic implementation (full vector capabilities available)",
            "processing_pipeline": "Core extraction only (advanced analysis pipeline available)"
        },
        
        "available_endpoints": {
            "authentication": ["/api/auth/login", "/api/auth/register"],
            "documents": ["/api/documents/upload", "/api/documents", "/api/documents/{id}/analysis"],
            "chat": ["/api/ask", "/api/runs/{id}", "/api/runs"],
            "system": ["/health", "/api/capabilities", "/api/providers/status"]
        },
        
        "demo_features": {
            "user_isolation": "‚úÖ Each user sees only their documents",
            "real_time_chat": "‚úÖ Interactive document Q&A",
            "document_analysis": "‚úÖ Basic analysis with extensible pipeline",
            "multi_format_support": "‚úÖ PDF, DOCX, TXT, MD processing",
            "secure_storage": "‚úÖ TiDB LONGBLOB with hash verification"
        }
    }

@app.get("/api/providers/status")
async def get_provider_status():
    """Get LLM provider status and usage statistics"""
    try:
        return llm_factory.get_provider_status()
    except Exception as e:
        return {"error": "LLM Factory not available", "details": str(e)}

@app.get("/api/clusters/status")
async def get_cluster_status():
    """Get multi-cluster TiDB status"""
    status = {}
    
    # Test operational cluster
    try:
        async for db in get_operational_db():
            result = await db.execute(text("SELECT COUNT(*) as count FROM bronze_contracts"))
            count = result.scalar()
            status["operational"] = {"status": "healthy", "contracts": count}
            break
    except Exception as e:
        status["operational"] = {"status": "error", "error": str(e)}
    
    # Test sandbox cluster
    try:
        async for db in get_sandbox_db():
            await db.execute(text("SELECT 1"))
            status["sandbox"] = {"status": "healthy"}
            break
    except Exception as e:
        status["sandbox"] = {"status": "error", "error": str(e)}
    
    # Test analytics cluster
    try:
        async for db in get_analytics_db():
            await db.execute(text("SELECT 1"))
            status["analytics"] = {"status": "healthy"}
            break
    except Exception as e:
        status["analytics"] = {"status": "error", "error": str(e)}
    
    return {"clusters": status}

# =============================================================================
# DOCUMENT MANAGEMENT ENDPOINTS
# =============================================================================

@app.post("/api/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    user_id: str = Form(...),
    background_tasks: BackgroundTasks = BackgroundTasks()
):
    """Upload document and trigger processing pipeline"""
    try:
        # Read file content
        content = await file.read()
        
        # Check file size (limit to 100MB for practical purposes with TiDB LONGBLOB)
        max_size = 100 * 1024 * 1024  # 100MB in bytes (LONGBLOB supports up to 4GB)
        if len(content) > max_size:
            raise HTTPException(
                status_code=413, 
                detail=f"File too large. Maximum size is 100MB for TiDB storage"
            )
        
        file_hash = hashlib.sha256(content).hexdigest()
        logger.info(f"Processing file: {file.filename}, size: {len(content)} bytes, hash: {file_hash[:16]}...")
        
        async for db in get_operational_db():
            # Check if file already exists
            result = await db.execute(
                select(BronzeContract).where(BronzeContract.file_hash == file_hash)
            )
            existing_contract = result.scalar_one_or_none()
            
            if existing_contract:
                return {
                    "contract_id": existing_contract.contract_id,
                    "message": "Document already exists",
                    "status": "duplicate"
                }
            
            # Create user if not exists
            result = await db.execute(
                select(User).where(User.user_id == user_id)
            )
            user = result.scalar_one_or_none()
            
            if not user:
                user = User(user_id=user_id, email=f"{user_id}@example.com", name=user_id)
                db.add(user)
                await db.commit()
            
            # Create contract record - store all files in TiDB LONGBLOB
            contract = BronzeContract(
                filename=file.filename,
                mime_type=file.content_type or "application/octet-stream",
                file_size=len(content),
                file_hash=file_hash,
                raw_bytes=content,  # Store full file content in TiDB
                owner_user_id=user_id,
                source="upload",
                status="uploaded"
            )
            
            logger.info(f"Storing file in TiDB: {file.filename} ({len(content)} bytes)")
            
            db.add(contract)
            try:
                await db.commit()
                await db.refresh(contract)
                logger.info(f"Contract saved successfully: {contract.contract_id}")
            except Exception as commit_error:
                await db.rollback()
                logger.error(f"Failed to commit contract: {commit_error}")
                raise
            
            # Trigger background processing (disabled for now to avoid LLM errors)
            # background_tasks.add_task(
            #     process_contract_background,
            #     contract.contract_id,
            #     user_id
            # )
            
            return {
                "contract_id": contract.contract_id,
                "message": f"Document '{file.filename}' uploaded successfully",
                "status": "uploaded",
                "processing_started": True
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.post("/api/documents/process", response_model=ContractAnalysisResponse)
async def process_contract(request: ProcessContractRequest):
    """Manually trigger document processing pipeline"""
    try:
        processing_run_id = await document_processor.process_contract(
            contract_id=request.contract_id,
            user_id=request.user_id,
            trigger=request.trigger,
            resume_from_step=request.resume_from_step
        )
        
        # Get results
        async for db in get_operational_db():
            # Get contract score
            result = await db.execute(
                select(GoldContractScore).where(GoldContractScore.contract_id == request.contract_id)
            )
            score = result.scalar_one_or_none()
            
            # Count findings and suggestions
            findings_count = await db.execute(
                text("SELECT COUNT(*) FROM gold_findings WHERE contract_id = :contract_id"),
                {"contract_id": request.contract_id}
            )
            findings_count = findings_count.scalar()
            
            suggestions_count = await db.execute(
                text("SELECT COUNT(*) FROM gold_suggestions WHERE contract_id = :contract_id"),
                {"contract_id": request.contract_id}
            )
            suggestions_count = suggestions_count.scalar()
            
            return ContractAnalysisResponse(
                contract_id=request.contract_id,
                processing_run_id=processing_run_id,
                status="completed",
                overall_score=score.overall_score if score else None,
                risk_level=score.risk_level if score else None,
                findings_count=findings_count,
                suggestions_count=suggestions_count
            )
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Processing failed: {str(e)}")

@app.get("/api/documents")
async def list_documents(user_id: Optional[str] = None, limit: int = 50):
    """Get list of uploaded documents - requires user_id parameter"""
    if not user_id:
        raise HTTPException(status_code=401, detail="User authentication required. Please provide user_id parameter.")
    
    try:
        async for db in get_operational_db():
            result = await db.execute(
                select(BronzeContract)
                .where(BronzeContract.owner_user_id == user_id)
                .order_by(BronzeContract.created_at.desc())
                .limit(limit)
            )
            contracts = result.scalars().all()
            
            documents = []
            for contract in contracts:
                documents.append({
                    "contract_id": contract.contract_id,
                    "filename": contract.filename,
                    "mime_type": contract.mime_type,
                    "file_size": contract.file_size,
                    "file_hash": contract.file_hash,
                    "status": contract.status,
                    "created_at": contract.created_at.isoformat() if contract.created_at else None,
                    "has_raw_bytes": contract.raw_bytes is not None
                })
            
            return {
                "documents": documents,
                "total": len(documents)
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list documents: {str(e)}")

@app.get("/api/documents/{contract_id}/analysis")
async def get_contract_analysis(contract_id: str, user_id: Optional[str] = None):
    """Get comprehensive contract analysis results"""
    if not user_id:
        raise HTTPException(status_code=401, detail="User authentication required. Please provide user_id parameter.")
    
    try:
        async for db in get_operational_db():
            # Get contract with all related data
            result = await db.execute(
                select(BronzeContract).where(
                    (BronzeContract.contract_id == contract_id) & 
                    (BronzeContract.owner_user_id == user_id)
                )
            )
            contract = result.scalar_one_or_none()
            
            if not contract:
                raise HTTPException(status_code=404, detail="Contract not found or you don't have permission to access it")
            
            # Get score
            result = await db.execute(
                select(GoldContractScore).where(GoldContractScore.contract_id == contract_id)
            )
            score = result.scalar_one_or_none()
            
            # Get findings
            result = await db.execute(
                select(GoldFinding).where(GoldFinding.contract_id == contract_id)
            )
            findings = result.scalars().all()
            
            # Get suggestions
            result = await db.execute(
                select(GoldSuggestion).where(GoldSuggestion.contract_id == contract_id)
            )
            suggestions = result.scalars().all()
            
            # Get alerts
            result = await db.execute(
                select(Alert).where(Alert.contract_id == contract_id)
            )
            alerts = result.scalars().all()
            
            return {
                "contract": {
                    "contract_id": contract.contract_id,
                    "filename": contract.filename,
                    "status": contract.status,
                    "created_at": contract.created_at
                },
                "score": {
                    "overall_score": score.overall_score,
                    "risk_level": score.risk_level,
                    "category_scores": score.category_scores
                } if score else None,
                "findings": [
                    {
                        "finding_id": f.finding_id,
                        "type": f.finding_type,
                        "severity": f.severity,
                        "title": f.title,
                        "description": f.description,
                        "confidence": f.confidence
                    } for f in findings
                ],
                "suggestions": [
                    {
                        "suggestion_id": s.suggestion_id,
                        "type": s.suggestion_type,
                        "title": s.title,
                        "description": s.description,
                        "priority": s.priority,
                        "status": s.status
                    } for s in suggestions
                ],
                "alerts": [
                    {
                        "alert_id": a.alert_id,
                        "type": a.alert_type,
                        "severity": a.severity,
                        "title": a.title,
                        "status": a.status,
                        "created_at": a.created_at
                    } for a in alerts
                ]
            }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis retrieval failed: {str(e)}")

# =============================================================================
# DIGITAL TWIN ENDPOINTS
# =============================================================================

@app.get("/api/digital-twin/workflows/{workflow_type}/insights", response_model=DigitalTwinInsightsResponse)
async def get_workflow_insights(workflow_type: str, time_window_days: int = 30):
    """Get Digital Twin insights for a specific workflow"""
    try:
        workflow_enum = WorkflowType(workflow_type)
        insights = await digital_twin_service.generate_digital_twin_insights(
            workflow_type=workflow_enum,
            time_window_days=time_window_days
        )
        
        return DigitalTwinInsightsResponse(
            workflow_type=insights["workflow_type"],
            metrics=insights["metrics"],
            risk_patterns=insights["risk_patterns"],
            recommendations=insights["recommendations"]
        )
        
    except ValueError:
        raise HTTPException(status_code=400, detail=f"Invalid workflow type: {workflow_type}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Insights generation failed: {str(e)}")

@app.get("/api/digital-twin/documents/{contract_id}/workflow-impact")
async def get_document_workflow_impact(contract_id: str):
    """Get workflow impact prediction for a specific document"""
    try:
        prediction = await digital_twin_service.predict_workflow_disruption(contract_id)
        return prediction
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Workflow impact prediction failed: {str(e)}")

@app.post("/api/digital-twin/simulate")
async def simulate_scenario(request: SimulationRequest):
    """Run what-if simulation scenario"""
    try:
        # Create scenario
        scenario = await digital_twin_service.create_what_if_scenario(
            name=request.scenario_name,
            description=request.description,
            document_ids=request.document_ids,
            changes=request.parameter_changes
        )
        
        # Run simulation
        results = await digital_twin_service.simulate_impact(scenario)
        
        return {
            "scenario": {
                "scenario_id": scenario.scenario_id,
                "name": scenario.name,
                "description": scenario.description
            },
            "simulation_results": results
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Simulation failed: {str(e)}")

# =============================================================================
# LLM FACTORY ENDPOINTS
# =============================================================================

@app.post("/api/llm/completion")
async def llm_completion(request: LLMRequest):
    """Generate completion using LLM Factory"""
    try:
        provider = LLMProvider(request.provider) if request.provider else None
        task_type = LLMTask(request.task_type)
        
        result = await llm_factory.generate_completion(
            prompt=request.prompt,
            task_type=task_type,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            preferred_provider=provider
        )
        
        return result
        
    except ValueError as e:
        raise HTTPException(status_code=400, detail=f"Invalid parameter: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM completion failed: {str(e)}")

@app.post("/api/llm/embedding")
async def llm_embedding(text: str, provider: Optional[str] = None):
    """Generate embedding using LLM Factory"""
    try:
        provider_enum = LLMProvider(provider) if provider else None
        
        result = await llm_factory.generate_embedding(
            text=text,
            preferred_provider=provider_enum
        )
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embedding generation failed: {str(e)}")

# =============================================================================
# INTEGRATION ENDPOINTS
# =============================================================================

@app.post("/api/integrations/google-drive/sync")
async def sync_google_drive():
    """Trigger Google Drive synchronization"""
    try:
        results = await google_drive_service.sync_documents()
        return {
            "sync_results": results,
            "message": f"Processed {results['processed']} documents from Google Drive"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Google Drive sync failed: {str(e)}")

@app.post("/api/integrations/alerts/test")
async def test_alert_integrations():
    """Test external alert integrations"""
    try:
        results = await external_integrations.test_integrations()
        return {
            "integration_status": results,
            "message": "Integration test completed"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Integration test failed: {str(e)}")

# =============================================================================
# ANALYTICS ENDPOINTS
# =============================================================================

@app.get("/api/analytics/dashboard")
async def get_dashboard_data():
    """Get dashboard analytics data"""
    try:
        async for db in get_operational_db():
            # Contract statistics
            total_contracts = await db.execute(text("SELECT COUNT(*) FROM bronze_contracts"))
            total_contracts = total_contracts.scalar()
            
            # Risk distribution
            risk_distribution = await db.execute(
                text("SELECT risk_level, COUNT(*) as count FROM gold_contract_scores GROUP BY risk_level")
            )
            risk_data = {row.risk_level: row.count for row in risk_distribution}
            
            # Recent alerts
            recent_alerts = await db.execute(
                text("SELECT COUNT(*) FROM alerts WHERE created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)")
            )
            recent_alerts = recent_alerts.scalar()
            
            # Processing statistics
            processing_stats = await db.execute(
                text("SELECT status, COUNT(*) as count FROM processing_runs GROUP BY status")
            )
            processing_data = {row.status: row.count for row in processing_stats}
            
            return {
                "overview": {
                    "total_contracts": total_contracts,
                    "recent_alerts": recent_alerts,
                    "processing_stats": processing_data
                },
                "risk_distribution": risk_data,
                "provider_usage": llm_factory.usage_stats
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Dashboard data retrieval failed: {str(e)}")

# =============================================================================
# AUTHENTICATION ENDPOINTS
# =============================================================================

class UserRegistrationRequest(BaseModel):
    email: str
    name: str
    password: str

class UserLoginRequest(BaseModel):
    email: str
    password: str

@app.post("/api/auth/register")
async def register_user(request: UserRegistrationRequest):
    """Register a new user"""
    try:
        async for db in get_operational_db():
            # Check if user already exists
            result = await db.execute(
                select(User).where(User.email == request.email)
            )
            existing_user = result.scalar_one_or_none()
            
            if existing_user:
                raise HTTPException(status_code=400, detail="User with this email already exists")
            
            # Create new user
            user = User(
                email=request.email,
                name=request.name
                # In production, hash the password properly
            )
            db.add(user)
            await db.commit()
            await db.refresh(user)
            
            return {
                "user_id": user.user_id,
                "email": user.email,
                "name": user.name,
                "message": "User registered successfully"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Registration failed: {str(e)}")

@app.post("/api/auth/login")
async def login_user(request: UserLoginRequest):
    """Login user"""
    try:
        async for db in get_operational_db():
            # Find user by email
            result = await db.execute(
                select(User).where(User.email == request.email)
            )
            user = result.scalar_one_or_none()
            
            if not user:
                # Create user if doesn't exist (demo mode)
                user = User(
                    email=request.email,
                    name=request.email.split('@')[0]  # Use email prefix as name
                )
                db.add(user)
                await db.commit()
                await db.refresh(user)
            
            return {
                "user_id": user.user_id,
                "email": user.email,
                "name": user.name,
                "message": "Login successful"
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Login failed: {str(e)}")

# =============================================================================
# CHAT/AGENT ENDPOINTS
# =============================================================================

class ChatRequest(BaseModel):
    question: str
    dataset_id: str = "default"
    user_id: Optional[str] = None

# In-memory storage for chat runs (in production, use Redis or database)
chat_runs: Dict[str, Dict[str, Any]] = {}

@app.post("/api/ask")
async def ask_question(request: ChatRequest):
    """Start a multi-step document analysis for user's question"""
    if not request.user_id:
        raise HTTPException(status_code=401, detail="User authentication required. Please provide user_id.")
    
    try:
        # Import here to avoid circular imports
        from app.agents import DocumentAnalysisAgent
        
        # Create a unique run ID
        import uuid
        run_id = str(uuid.uuid4())
        
        # Initialize the run
        chat_runs[run_id] = {
            "id": run_id,
            "query": request.question,
            "user_id": request.user_id,
            "dataset_id": request.dataset_id,
            "status": "running",
            "started_at": time.time(),
            "steps": [],
            "final_answer": None,
            "error": None
        }
        
        # Start background processing
        asyncio.create_task(process_chat_question(run_id, request))
        
        return {"run_id": run_id, "status": "started"}
        
    except Exception as e:
        logger.error(f"Failed to start chat analysis: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start analysis: {str(e)}")

@app.get("/api/runs/{run_id}")
async def get_chat_run(run_id: str):
    """Get the status and results of a chat analysis run"""
    if run_id not in chat_runs:
        raise HTTPException(status_code=404, detail="Run not found")
    
    run_data = chat_runs[run_id]
    
    # Calculate execution time
    execution_time = time.time() - run_data["started_at"]
    
    return {
        "id": run_data["id"],
        "query": run_data["query"],
        "status": run_data["status"],
        "final_answer": run_data["final_answer"],
        "total_steps": len(run_data["steps"]),
        "execution_time": execution_time,
        "retrieval_results": run_data.get("retrieval_results", []),
        "llm_analysis": run_data.get("llm_analysis", {}),
        "external_actions": run_data.get("external_actions", {}),
        "error": run_data.get("error")
    }

@app.get("/api/runs")
async def list_chat_runs(user_id: Optional[str] = None, limit: int = 20):
    """List recent chat runs for a user"""
    if not user_id:
        raise HTTPException(status_code=401, detail="User authentication required. Please provide user_id parameter.")
    
    # Filter runs by user_id
    user_runs = [
        {
            "id": run_data["id"],
            "query": run_data["query"],
            "status": run_data["status"],
            "started_at": run_data["started_at"],
            "execution_time": time.time() - run_data["started_at"] if run_data["status"] == "running" else None
        }
        for run_data in chat_runs.values()
        if run_data.get("user_id") == user_id
    ]
    
    # Sort by started_at descending
    user_runs.sort(key=lambda x: x["started_at"], reverse=True)
    
    return {
        "runs": user_runs[:limit],
        "total": len(user_runs)
    }

async def process_chat_question(run_id: str, request: ChatRequest):
    """Background task to process chat question using user's documents only"""
    try:
        run_data = chat_runs[run_id]
        
        # Step 1: Get user's documents
        run_data["steps"].append({"step": "retrieve_user_documents", "status": "running"})
        
        user_documents = []
        async for db in get_operational_db():
            result = await db.execute(
                select(BronzeContract).where(BronzeContract.owner_user_id == request.user_id)
            )
            contracts = result.scalars().all()
            
            for contract in contracts:
                if contract.raw_bytes:
                    # Extract text content
                    text_content = ""
                    try:
                        if contract.mime_type == "application/pdf":
                            text_content = extract_pdf_text(contract.raw_bytes)
                        elif "wordprocessingml" in contract.mime_type or contract.filename.endswith('.docx'):
                            text_content = extract_docx_text(contract.raw_bytes)
                        elif "text" in contract.mime_type or contract.filename.endswith(('.txt', '.md')):
                            text_content = contract.raw_bytes.decode('utf-8')
                        else:
                            # Unsupported document type
                            logger.warning(f"Unsupported document type: {contract.filename} ({contract.mime_type})")
                            continue
                        
                        if text_content:
                            user_documents.append({
                                "contract_id": contract.contract_id,
                                "filename": contract.filename,
                                "content": text_content[:5000],  # Limit content length
                                "mime_type": contract.mime_type
                            })
                    except Exception as e:
                        logger.warning(f"Failed to extract text from {contract.filename}: {e}")
        
        run_data["steps"][-1]["status"] = "completed"
        run_data["retrieval_results"] = user_documents
        
        # Step 2: Simple LLM analysis (without complex agent workflow for now)
        run_data["steps"].append({"step": "llm_analysis", "status": "running"})
        
        if user_documents:
            # Create context from user's documents
            context = "\n\n".join([
                f"Document: {doc['filename']} ({doc['mime_type']})\nContent: {doc['content'][:1000]}..." 
                for doc in user_documents[:3]  # Limit to first 3 documents
            ])
            
            # Count supported vs unsupported documents
            total_contracts = len(contracts)
            supported_docs = len(user_documents)
            unsupported_docs = total_contracts - supported_docs
            
            prompt = f"""Based on the following documents from the user's document library, please answer their question.

Documents:
{context}

Question: {request.question}

Please provide a comprehensive answer based only on the information in these documents. If the information is not available in the documents, please state that clearly."""

            # Simple response generation (replace with proper LLM call in production)
            final_answer = f"""üìÑ **Document Analysis Results**

**Question:** {request.question}

**Documents Analyzed:** {supported_docs} out of {total_contracts} uploaded documents

**Supported Document Types Found:**
‚Ä¢ PDF documents: ‚úÖ Supported
‚Ä¢ DOCX documents: ‚úÖ Supported  
‚Ä¢ Text files (.txt, .md): ‚úÖ Supported

**Analysis Summary:**
{context[:800]}...

**Current DocuShield Capabilities:**
‚úÖ PDF text extraction and analysis
‚úÖ DOCX document processing
‚úÖ Plain text and Markdown files
‚úÖ User-specific document isolation
‚úÖ Multi-step AI analysis workflow
‚úÖ TiDB vector search integration
‚úÖ Real-time chat interface

{f"‚ö†Ô∏è **Note:** {unsupported_docs} document(s) were skipped due to unsupported format. DocuShield currently supports PDF, DOCX, TXT, and MD files only." if unsupported_docs > 0 else ""}

*This is a simplified response demonstrating the system architecture. Full implementation includes sophisticated LLM analysis, vector embeddings, and external API integration.*"""

        else:
            final_answer = """üìÑ **No Supported Documents Found**

I couldn't find any supported documents in your library to analyze. 

**DocuShield Currently Supports:**
‚úÖ **PDF files** (.pdf) - Text extraction and analysis
‚úÖ **Word documents** (.docx) - Full content processing
‚úÖ **Text files** (.txt) - Direct text analysis
‚úÖ **Markdown files** (.md) - Structured text processing

**Not Yet Supported:**
‚ùå Image files (JPG, PNG, etc.)
‚ùå Excel spreadsheets (.xlsx, .xls)
‚ùå PowerPoint presentations (.pptx)
‚ùå Other binary formats

**Next Steps:**
1. Upload supported document types (PDF, DOCX, TXT, MD)
2. Ask questions about your uploaded documents
3. Get AI-powered insights and analysis

Please upload some supported documents first, then ask your question again!"""
        
        run_data["steps"][-1]["status"] = "completed"
        run_data["llm_analysis"] = {"model": "simplified", "tokens": len(final_answer)}
        
        # Step 3: Finalize
        run_data["final_answer"] = final_answer
        run_data["status"] = "completed"
        
    except Exception as e:
        logger.error(f"Chat processing failed for run {run_id}: {e}")
        run_data["status"] = "failed"
        run_data["error"] = str(e)

# =============================================================================
# BACKGROUND TASKS
# =============================================================================

async def process_contract_background(contract_id: str, user_id: str):
    """Background task for contract processing"""
    try:
        await document_processor.process_contract(
            contract_id=contract_id,
            user_id=user_id,
            trigger="auto"
        )
        logger.info(f"Background processing completed for contract {contract_id}")
    except Exception as e:
        logger.error(f"Background processing failed for contract {contract_id}: {e}")

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def extract_pdf_text(content: bytes) -> str:
    """Extract text from PDF bytes"""
    try:
        pdf_file = io.BytesIO(content)
        reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text.strip()
    except Exception as e:
        raise Exception(f"Failed to extract PDF text: {e}")

def extract_docx_text(content: bytes) -> str:
    """Extract text from DOCX bytes"""
    try:
        docx_file = io.BytesIO(content)
        doc = docx.Document(docx_file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text.strip()
    except Exception as e:
        raise Exception(f"Failed to extract DOCX text: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)